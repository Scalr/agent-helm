{{ template "chart.header" . }}
{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.description" . }}

> [!WARNING]
> This chart is in Alpha, and implementation details are subject to change.

## Prerequisites

- Kubernetes 1.33+ (require [sidecar containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/))
- Helm 3.0+
- ReadWriteMany volumes for [cache cersistance](#cache-directory-persistence) (optional)

## Installation

To install the chart with the release name `scalr-agent`:

```bash
# Add the Helm repo
helm repo add scalr-agent https://scalr.github.io/agent-helm/
helm repo update

# Install or upgrade the chart
helm upgrade --install scalr-agent scalr-agent/{{ template "chart.name" . }} \
  --set agent.token="<agent-pool-token>"
```

## Overview

The `agent-job` Helm chart deploys a [Scalr Agent](https://docs.scalr.io/docs/agent-pools) that uses a job-based architecture for executing Terraform/OpenTofu infrastructure tasks in Kubernetes.

The chart consists of two Kubernetes resources: **[agent](#agent)** and **[agent task](#agent-task)**.

### Agent

The agent is a [Kubernetes Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), deployed by default as a single replica consisting of one container:

- **controller**: The controller that creates Kubernetes Jobs, based on the `scalr/agent` image.

The agent controller is responsible for polling incoming tasks from Scalr and launching them as isolated Kubernetes Jobs.

See [template](https://github.com/Scalr/agent-helm/blob/master/charts/agent-job/templates/agent.yaml).

### Agent Task

Each agent task is a [Kubernetes Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) created by the agent controller. It consists of two isolated containers:

- **runner**: The environment where the run (Terraform/OpenTofu operations, OPA policies, shell hooks, etc.) is executed, based on the [scalr/runner](https://hub.docker.com/r/scalr/runner) image.
- **worker** (sidecar): The Scalr Agent process that supervises task execution, using the [scalr/agent](https://hub.docker.com/r/scalr/agent) image.

The task template is defined via a [Custom Resource Definition](#custom-resource-definitions). The agent **controller** uses this resource to create Jobs from a template fully managed by this Helm chart. The controller may patch the Job definition to inject dynamic resources, such as labels and annotations with resource IDs (run ID, workspace ID, etc.).

The runner and worker containers share a single disk volume, allowing the worker to provision the configuration version, providers, and software binaries required by the runner container.

The number of agent task Jobs depends on the active workload that the Scalr platform delegates to the agent pool to which the agent is connected.

See [template](https://github.com/Scalr/agent-helm/blob/master/charts/agent-job/templates/task.yaml).

### Pros

- Cost-efficient for bursty workloads — e.g., deployments with high number of Runs during short periods and low activity otherwise, as resources allocated on demand for each Scalr Run.
- High multi-tenant isolation, as each Scalr Run always has its own newly provisioned environment.
- Better observability, as each Scalr Run is tied to its own unique Pod.

### Cons

- Requires access to the Kubernetes API to launch new Pods.
- Requires a ReadWriteMany Persistent Volume configuration for provider/binary caching. This type of volume is generally vendor-specific and not widely available across all cloud providers.

## Architecture Diagram

<p align="center">
  <img src="assets/deploy-diagram.drawio.svg" />
</p>

## Custom Runner Images

You can override `task.runner.image.*` to use a custom runner image.
If you are using a custom runner image, it must include a user with UID/GID `1000`. By default, Scalr images come with a user `scalr` under `1000:1000`.

Example override:

```console
helm upgrade --install scalr-agent scalr-charts/{{ template "chart.name" . }} \
  --set agent.token="<agent-token>" \
  --set task.runner.image.repository="registry.example.com/custom-runner" \
  --set task.runner.image.tag="v1.2.3"
```

## Cache Directory Persistence

The *cache directory* stores provider binaries, plugin cache, and downloaded tools. This volume is mounted to both the worker (full access) and runner (some direcrotires, in readonly) containers.

Default configuration uses ephemeral `emptyDir` storage. Each task will download providers and binaries fresh.

It's recommended to enable persistent storage with `ReadWriteMany` access mode to share the cache across all task pods. This significantly improves performance by avoiding repeated downloads (saves 1-5 minutes per task).

Benefits of persistent cache:

- Faster task execution (no provider/binaries re-downloads)
- Reduced network bandwidth usage

When enabling a persistent cache directory, it is recommended to also enable provider cache (`providerCache.enabled=true`). Learn more about [Provider Cache](https://docs.scalr.io/docs/providers-cache). Otherwise, only software binaries (Terraform/OpenTofu/OPA/Infracost/etc.) will be cached.

**Configuration Example with PVC**:

```yaml
persistence:
  cache:
    enabled: true
    persistentVolumeClaim:
      # Use existing PVC
      claimName: "my-cache-pvc"
      # Or create new PVC (omit claimName)
      storageClassName: "nfs-client"
      storage: 40Gi
      accessMode: ReadWriteMany
agent:
  providerCache:
    enabled: true
    sizeLimit: 20Gi # soft-limit
```

## Data Directory Persistence

The *data directory* stores temporary workspace data needed for processing a run, including run metadata and source code.

The default configuration uses ephemeral `emptyDir` storage. Since the workspace volume does not need to be shared or persisted between runs, we recommend using an ephemeral volume so that it is bound to the lifetime of the run and automatically destroyed when the Job is deleted.

Optionally, you can configure a PVC using `persistence.data.enabled` and `persistence.data.persistentVolumeClaim` options, similar to the [*cache directory* configuration](#cache-directory-persistence).

## Security Features

### Runner Security Context

Runner pods inherit their Linux user, group, seccomp, and capability settings from `task.runner.securityContext`. The defaults run the container as the non-root UID/GID `1000`, drop all Linux capabilities, and enforce a read-only root filesystem.

The default is strict and compatible with Terraform/OpenTofu workloads, and it’s generally not recommended to change it. However, it can be useful to disable `readOnlyRootFilesystem` and switch the user to root if you need to install packages via package managers like `apt-get` or `dnf` from Workspace hooks.

### Restrict Access to VM Metadata Service

The chart includes a feature to restrict task pods from accessing the VM metadata service at 169.254.169.254, which is common for both AWS and GCP environments.

By default this option is enabled, and a Kubernetes NetworkPolicy is applied to task pods that denies egress traffic to 169.254.169.254/32, blocking access to the VM metadata service. All other outbound traffic is allowed.

To disable this restriction, set `task.allowMetadataService` to `true`:

```console
$~ helm upgrade ... \
    --set task.allowMetadataService=true
```

**Note**: The controller pod is not affected by this NetworkPolicy and retains full network access.

#### Limitations

Ensure that your cluster is using a CNI plugin that supports egress NetworkPolicies. Example: Calico, Cilium, or native GKE NetworkPolicy provider for supported versions.

If your cluster doesn't currently support egress NetworkPolicies, you may need to recreate it with the appropriate settings.

## Job History Management

Kubernetes automatically removes Jobs after `task.job.ttlSecondsAfterFinished` seconds (default: 60). Increase this value for debugging or to preserve job history longer, or decrease it to optimize cluster resource usage.

## Metrics and Observability

The agent can be configured to send telemetry data, including both trace spans and metrics, using [OpenTelemetry](https://opentelemetry.io/).

OpenTelemetry is an extensible, open-source telemetry protocol and platform that enables the Scalr Agent to remain vendor-neutral while producing telemetry data for a wide range of platforms.

Enable telemetry for both the controller deployment and the worker sidecar by configuring an OpenTelemetry collector endpoint:

```yaml
otel:
  enabled: true
  endpoint: "otel-collector:4317"  # gRPC endpoint
  metricsEnabled: true
  tracesEnabled: false  # Optional: enable distributed tracing
```

See [all configuration options](#opentelemetry).

Learn more about [available metrics](https://docs.scalr.io/docs/metrics).

## Custom Resource Definitions

This chart bundles the **AgentTask CRD** (`atasks.scalr.io`) and installs or upgrades it automatically via Helm. The CRD defines the job template that the controller uses to create task pods, so no separate manual step is required in most environments.

**Verify installation:**

```console
kubectl get crd atasks.scalr.io
```

## RBAC

By default the chart provisions:

- **ServiceAccount** used by the controller and task pods
- **Role/RoleBinding** with namespaced access to manage pods/jobs and related resources needed for task execution
- **ClusterRole/RoleBinding** granting read access to `AgentTask` resources (`atasks.scalr.io`)

Set `rbac.create=false` to bring your own ServiceAccount/Rules, or adjust permissions with `rbac.rules` and `rbac.clusterRules`.

## Troubleshooting and Support

### Debug Logging

If you encounter internal system errors or unexpected behavior, enable debug logs:

```console
helm upgrade scalr-agent scalr-agent-helm/{{ template "chart.name" . }} \
  --reuse-values \
  --set agent.debug="1"
```

Then collect logs ([see below](#collecting-logs)) and open a support request at [Scalr Support Center](https://scalr-labs.atlassian.net/servicedesk/customer/portal/31).

### Collecting Logs

When inspecting logs, you'll need both the agent log (from the `scalr-agent-*` deployment pod) and the task log (from an `atask-*` job pod). Job pods are available for 60 seconds after completion. You may want to increase this time window using `task.job.ttlSecondsAfterFinished` to allow more time for log collection.

Use `kubectl logs` to retrieve logs from the `scalr-agent-*` and `atask-*` pods (if any):

```console
kubectl logs -n <namespace> <task-pod-name> --all-containers
```

### Getting Support

For issues not covered above:

1. Enable [debug logging](#debug-logging)
2. [Collect logs](#collecting-logs) from the incident timeframe
3. Open a support ticket at [Scalr Support Center](https://scalr-labs.atlassian.net/servicedesk/customer/portal/31)

{{ template "chart.maintainersSection" . }}

{{ template "chart.requirementsSection" . }}

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}
