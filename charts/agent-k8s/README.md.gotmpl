{{ template "chart.header" . }}
{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.description" . }}

## Table of Contents

- [Overview](#overview)
- [Deployment Diagram](#deployment-diagram)
- [Installation](#installation)
- [Performance Optimization](#performance-optimization)
- [Disk Requirements](#disk-requirements)
- [Choosing the Data Home Directory](#choosing-the-data-home-directory)
- [Amazon EFS](#amazon-efs)
- [Restrict Access to VM Metadata Service](#restrict-access-to-vm-metadata-service)
- [HTTP Proxy](#http-proxy)
- [SSL Certificate Bundles](#ssl-certificate-bundles)
- [Troubleshooting](#troubleshooting)
- [Limitations](#limitations)

## Overview

The Agent deploys as two components: a controller and a worker. The controller
consumes jobs from Scalr and schedules pods, while the worker supervises the jobs.

The agent worker is a DaemonSet that scales up/down with the cluster, registering
and deregistering agents from the pool. When an Agent controller receives a job from Scalr,
it schedules a Pod for execution. The Kubernetes workload scheduler assigns the Pod
to a specific Node, where the Agent worker running on that Node oversees the execution
of the job. By enabling the Kubernetes auto-scaler, Scalr Run workloads can scale
linearly based on the load.

### Pros

- Cost-efficient for bursty workloads — e.g., deployments with high number of Runs during short periods and low activity otherwise, as resources allocated on demand for each Scalr Run.
- High multi-tenant isolation, as each Scalr Run always has its own newly provisioned environment.
- Better observability, as each Scalr Run is tied to its own unique Pod.

### Cons

- Requires access to the Kubernetes API to launch new Pods.
- Requires a ReadWriteMany Persistent Volume configuration for provider/binary caching. This type of volume is generally vendor-specific and not widely available across all cloud providers.
- May spawn too many services without having its own dedicated node pool. [Details](#daemonset).
- Relies on a hostPath volume. [Details](#hostpath-volume).

## Deployment Diagram

<p align="center">
  <img src="assets/agent-k8s-deploy-diagram.jpg" />
</p>

## Installation

To install the chart with the release name `scalr-agent`:

```console
$ helm repo add scalr-agent-helm https://scalr.github.io/agent-helm/
$ helm upgrade --install scalr-agent scalr-agent-helm/{{ template "chart.name" . }} \
    --set agent.url="https://<account>.scalr.io" \
    --set agent.token="<agent-pool-token>"
```

You can also control the placement of both the controller and the worker on the cluster using the `controllerNodeSelector`
and `workerNodeSelector` options. Here's an example using GKE specific labels:

```console
$ helm upgrade --install scalr-agent scalr-agent-helm/{{ template "chart.name" . }}
    --set agent.url="https://<account>.scalr.io" \
    --set agent.token="<agent-pool-token>" \
    --set controllerNodeSelector."kubernetes\\.io\\/hostname"="<node-name>" \
    --set workerNodeSelector."cloud\\.google\\.com\\/gke-nodepool"="<node-pool-name>"
```

To use a separate agent pool for Scalr workloads, you may want to configure [Taint and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).
Set up the taints on the Node Pool, and add tolerations to the agent worker with the `workerTolerations` option. An example:

```console
--set workerTolerations[0].operator=Equal,workerTolerations[0].effect=NoSchedule,workerTolerations[0].key=dedicated,workerTolerations[0].value=scalr-agent-worker-pool
```

## Performance Optimization

The following additional configurations are recommended to optimize Scalr Run startup time and overall chart performance.

### Optimize Run Startup Time

This chart uses individual Pods to isolate Scalr runs and creates a new pod each time a Scalr run stage is queued. As a result, fast pod startup is critical for low run startup latency.
Common bottlenecks that may introduce latency include slow image pull times. To optimize this, you can:

- Use image copies in an OCI-compatible registry mirror (Google Container Registry, Amazon Elastic Container Registry, Azure Container Registry, and similar) located in the same region as your node pool. This enables faster pull times and reduces the risk of hitting Docker Hub rate limits.
Use the `container_task_image` or `container_task_image_registry` settings (the latter applies to the [older Docker-image–based software version integrations](https://docs.scalr.io/docs/run-environment#software-releases)) to configure the run image.
- Use a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) to preemptively cache all images used in this chart (`scalr/agent`, `scalr/runner`). Otherwise, runs on cold nodes will be delayed by image pull time.
- Enable [Image Streaming](https://docs.cloud.google.com/kubernetes-engine/docs/how-to/image-streaming) (GKE only) to improve Pod launch time.

The run startup performance also depends on node scaling speed when autoscaling is enabled.

### Resource Limits

Proper resource limit configuration is critical for achieving good run performance.
We recommend avoiding CPU limits for worker services, as they are installed per node (via a DaemonSet) and can process multiple runs concurrently, all scheduled on the same node.
Setting CPU limits on workers may lead to throttling and degraded performance under load.

For the run environment (the container where OpenTofu/Terraform is executed), resource requests and limits can be configured using the following Helm values:

```yaml
# -- CPU resource request, defined in cores.
# For example:
#   2.0  = two full CPU cores
#   0.25 = one quarter of a CPU core
agent.container_task_cpu_request: 1.0

# -- CPU resource limit, defined in cores.
# This is the maximum amount of CPU the container is allowed to use.
agent.container_task_cpu_limit: 8.0

# -- Memory resource request, defined in megabytes.
# This is the amount of memory reserved for the container.
agent.container_task_mem_request: 1024

# -- Memory resource limit, defined in megabytes.
# This is the maximum amount of memory the container can use before being terminated.
agent.container_task_mem_limit: 16384
```

## Disk Requirements

Currently, the Agent is not fully cloud-native and utilizes the [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)
volume for storing a shared OpenTofu/Terraform plugin cache and managing configuration version artifacts
for agent task Pods.

The volume is configured via the `agent.data_home` option. The filesystem on this volume must be
writable, executable, and stateful (within the lifecycle of the Scalr Agent Pod).

### Choosing the Data Home Directory

In the default template example, the node disk is utilized at the path `/home/kubernetes/flexvolume/agent-k8s`.
This path is specific to Container-Optimized OS (GKE) and varies depending on the Kubernetes provider in use.

There is also a known [issue](https://github.com/Scalr/agent-helm/pull/32) with the default `agent.data_home` directory, which will be changed in the future.

It is recommended to alter the default directory to `/home/kubernetes/bin/scalr/{unique-name}`.

For EKS (Amazon Linux 2 or Bottlerocket OS), the recommended path is `/var/lib/{unique-name}`.

Using a unique name in the path is necessary when installing multiple agents on the cluster
to prevent collisions. Additionally, it is important to note that the Agent does not delete its
data when uninstalling the chart or modifying the `agent.data_home` option, which may result
in artifacts being left on the node's root disk.

Example of setting `agent.data_home`:

```console
$ helm upgrade ... \
    --set agent.data_home="/var/lib/{unique-name}"
```

## Amazon EFS

Amazon EFS can be used as a shared ReadWriteMany volume instead of a node disk. To configure it,
install the `Amazon EFS CSI Driver` via an add-on. See the documentation: https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html#efs-install-driver.
Ensure the add-on is active before proceeding.

Next, configure the Amazon EFS file system ID using the `efsVolumeHandle` option:

```console
$ helm upgrade ... \
    --set agent.data_home="/var/lib/{unique-name}" \
    --set efsVolumeHandle="fs-582a03f3"
    # Alternatively, if using an Access Point:
    # see: https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html#accessing-fs-nfs-permissions-access-points
    --set efsVolumeHandle="fs-582a03f3::fsap-01e050b7d9a3109d5"
```

The EFS storage will be mounted in all worker containers at the `agent.data_home` path. All child containers
for Runs will inherit the EFS configuration. The controller will continue to use an ephemeral directory
as its data home.

The EFS configuration includes default mount options to ensure that the NFS attribute cache is kept minimal, providing better read-after-write consistency across pods:

```yaml
efsMountOptions:
  - acregmin=1
  - acregmax=3
  - acdirmin=1
  - acdirmax=3
```

Changing these defaults may affect Scalr Agent behavior. For more information, see: <https://www.ibm.com/docs/en/aix/7.2.0?topic=client-nfs-file-attribute-cache-tuning>

## Restrict Access to VM Metadata Service

The chart includes an optional feature to restrict the pods from accessing the VM metadata service at 169.254.169.254, that is common for both AWS and GCP environments.

To enable it, use the `restrictMetadataService` option:

```console
$ helm upgrade ... \
    --set restrictMetadataService=true
```

With this option enabled, a Kubernetes NetworkPolicy is applied to the agent pods that denies egress traffic to 169.254.169.254/32, blocking access to the VM metadata service. All other outbound traffic is allowed.

### HTTP Proxy

To configure an HTTP proxy, set the `HTTP_PROXY`, `HTTPS_PROXY`, and `NO_PROXY` environment variables.

Example of running agent with HTTP proxy enabled:

```console
$ helm upgrade ... \
  --set extraEnv.HTTP_PROXY="<proxy-address>" \
  --set extraEnv.HTTPS_PROXY="<proxy-address>" \
  --set extraEnv.NO_PROXY="<addr1>,<addr2>"
```

### SSL Certificate Bundles

To configure SSL certificates globally, use the `agent.ca_cert` option.
To configure SSL certificates only for isolated containers for the tasks (e.g. tofu/terraform/infracost operations), set the `agent.container_task_ca_cert` option.

You can bundle your certificate into an agent image. Place the custom CA file at `extra_ca_root.crt` and build the customized image:

```Dockerfile
FROM scalr/agent:latest

ADD extra_ca_root.crt /usr/local/share/ca-certificates/extra-ca.crt
RUN apt update \
    && apt install ca-certificates -y \
    && chmod 644 /usr/local/share/ca-certificates/extra-ca.crt \
    && update-ca-certificates
ENV SCALR_CA_CERT="/etc/ssl/certs/ca-certificates.crt" # same as `agent.ca_cert` helm option
# ENV SCALR_CONTAINER_TASK_CA_CERT="/etc/ssl/certs/ca-certificates.crt" # same as `agent.container_task_ca_cert` helm option
```

This step also bundles your certificate with the set of public certificates provided by `ca-certificates`
system package. You can optionally skip this step and instead point `SCALR_CA_CERT` (or `SCALR_CONTAINER_TASK_CA_CERT`) to your certificate
if it already includes public CA certificates or if they are not needed (e.g., in a setup completely hidden behind a proxy).

Note that by default, the scalr agent uses the certificate bundle provided by the [certifi](https://github.com/certifi/python-certifi) package instead of the system certificate bundle provided by the `ca-certificates` package.

Alternatively, a base64-encoded string containing the certificate bundle can be used.

Example of encoding a bundle:

```console
$~ cat /path/to/bundle.ca | base64
```

Example of running agent with custom CA certifcates:

```console
$ helm upgrade ... \
  -e agent.ca_cert=<base-64-encoded-certs>
```

### Troubleshooting

If you encounter internal system errors or unexpected behavior, please open a Scalr Support request at [Scalr Support Center](https://scalr-labs.atlassian.net/servicedesk/customer/portal/31).

Before doing so, enable debug logs using the `agent.debug` option. Then collect the debug-level application logs covering the time window when the incident occurred, and attach them to your support ticket.

This chart uses a controller-worker model with one controller pod and zero or more worker pods. Be sure to include logs from at least the controller pod and any affected worker pods.

To archive all logs from the Scalr agent namespace in a single bundle, replace the `ns` variable with the name of your Helm release namespace and run:

```shell
ns="scalr-agent"
mkdir -p logs && for pod in $(kubectl get pods -n $ns -o name); do kubectl logs -n $ns $pod > "logs/${pod##*/}.log"; done && zip -r agent-k8s-logs.zip logs && rm -rf logs
```

It's best to pull the logs immediately after an incident, since this command will not retrieve logs from restarted or terminated pods.

### Limitations

Ensure that your cluster is using a CNI plugin that supports egress NetworkPolicies. Example: Calico, Cilium, or native GKE NetworkPolicy provider for supported versions.

If your cluster doesn't currently support egress NetworkPolicies, you may need to recreate it with the appropriate settings.

### Issues

This implementation has several design choices that may prevent adoption.

#### DaemonSet

Scalr Agents from the start were Docker-based and built with multi-tenancy in mind, designed to run and isolate concurrent Scalr Runs within a single agent instance, keeping OpenTofu/Terraform workloads separated by design. They are also built using third-party software bundled via Docker images (OpenTofu, Terraform, OPA, Infracost, etc.), which introduces a Docker dependency.

Our initial Kubernetes implementation followed the pattern introduced by the Docker-based agents. It uses a cloud-native controller/worker model.
The Agent Controller is deployed as a Deployment, while agent workers are deployed as a DaemonSet across all nodes in the cluster or a specific node pool.
The Agent Controller pulls tasks from Scalr and launches task pods to execute Run workflows. The DaemonSet ensures a single worker per node to handle multiple Run workflows and reduce resource usage.

The DaemonSet auto-scales workers across all nodes. This is a valid solution only if you have a dedicated cluster or at least a separate node pool. Otherwise, it may scale across a large number of nodes, spawning too many idle workers.

#### hostPath volume

Another important aspect of this implementation is the reliance on a [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume.
Since the Agent is based on the OpenTofu/Terraform architecture, which depends on plugins, each initialization triggers the download of all providers defined in the configuration.
These downloads can be very large, so local persistent storage was necessary to cache providers and avoid redownloading them for each Scalr Run stage. There’s no scalable way to
use local storage except through hostPath (per-node cache) or ReadWriteMany volumes, which are vendor-specific and complex to configure — making them impractical to provide out of the box.

The hostPath volume is unacceptable for many users. It’s also restricted by some Kubernetes vendors, such as GKE Autopilot, which enforces stricter limitations.

#### Solution

This issue is resolved by the [`agent-job`](/charts/agent-job) chart (Alpha).

{{ template "chart.maintainersSection" . }}

{{ template "chart.requirementsSection" . }}

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}
