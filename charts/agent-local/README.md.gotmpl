{{ template "chart.header" . }}
{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.description" . }}

> [!WARNING]
> This chart is compatible with Scalr Agent >= 0.45.0

## Overview

The agent-local chart deploys the Scalr Agent as a single Deployment with ephemeral storage for
provider and binary caching. Storage can optionally be upgraded to persistent volumes to maintain
cache persistence across pod restarts.

The chart uses the `local` Scalr Agent driver, where all operations are executed in local subprocesses.
As a result, it uses the `scalr/agent` image as the base environment for runs instead of [scalr/runner](https://docs.scalr.io/docs/self-hosted-agents-pools#golden-image-beta) image.

The concurrency of each agent instance is limited to 1. To scale concurrency, the recommended approach is to increase the `replicaCount`.

### Pros

- Simple to deploy.
- Scalr Agent service doesn’t require permissions to access the Kubernetes API.
- Includes Provider Cache and Binary Cache by default.

### Cons

- Doesn’t support autoscaling out of the box. You need manually increase or decrease the number of replicas or configure Horizontal Pod Autoscaler.
- Not cost-efficient for bursty workloads — e.g., deployments with high number of Runs during short periods and low activity otherwise, as resources remain allocated even when idle.
- Low multi-tenant isolation. A sequence of Scalr Runs shares the same container and data storage. This chart should only be used within a single RBAC perimeter and is unsuitable for untrusted environments.

## Deployment Diagram

<p align="center">
  <img src="assets/deploy-diagram.drawio.svg" />
</p>

## Installing

To install the chart with the release name `scalr-agent`:

```console
$ helm repo add scalr-charts https://scalr.github.io/agent-helm/
$ helm install scalr-agent scalr-charts/agent-local --set agent.token="<agent-token>"
```

_See [configuration](#values) below._

_See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation._

## Agent Configuration

The Scalr Agent is configured using environment variables, which can be set using the `extraEnv` option in the Helm chart.

```console
$ helm install ...
  --set extraEnv.SCALR_DEBUG=1 \
  --set extraEnv.HTTPS_PROXY="http://myproxy.com:3128"
```

## Customizing Environment

This chart uses the local driver to run tasks directly within the container where the agent operates. Therefore, it requires an image that includes both the Scalr Agent service and the additional tooling provided by the [scalr/runner](https://hub.docker.com/r/scalr/runner) image. As a result, this chart uses the [scalr/agent-runner](https://hub.docker.com/r/scalr/agent-runner) image, which combines the minimal Scalr Agent image ([scalr/agent](https://hub.docker.com/r/scalr/agent)) with the extra tools from `scalr/runner`. You can use this image, or `scalr/agent` (as a minimal base for building your own lightweight images), as a starting point for customizing your environment.

## Volume Configuration

The Scalr Agent uses a data volume for caching run data, configuration versions,
and OpenTofu/Terraform plugins, stored in the directory specified by `dataHome` (default: `/var/lib/scalr-agent`).
This directory is mounted to a volume defined in the `persistence` section. Since the data volume is used for temporary files and caching, both ephemeral and persistent storage are suitable for production. However, persistent storage avoids re-downloading providers and binaries on pod restarts and positively impacts Run Stage initialization times.

### Storage Options

- **`emptyDir`**: Ephemeral storage is enabled by default. Data is lost on pod restarts, requiring providers and binaries to be re-downloaded each time.
- **`persistentVolumeClaim` (PVC)**: Persistent storage suitable for retaining cache across restarts or sharing it between different Scalr Agent replicas. Supports both dynamic PVC creation and use of existing PVCs.

The volume is mounted at `dataHome`, which must be readable, writable, and executable for OpenTofu/Terraform plugin execution.

### Configuration Examples

#### Single Replica with Persistent Storage

Use `ReadWriteOnce` for single-replica deployments (`replicaCount: 1`):

```yaml
persistence:
  enabled: true
  persistentVolumeClaim:
    storageClassName: gp2
    storage: 10Gi
    accessMode: ReadWriteOnce
```

#### Multi-Replica with Shared Storage

For multiple replicas (`replicaCount: >1`) in clusters with `ReadWriteMany` support (e.g., NFS, AWS EFS), use a single shared PVC.

```yaml
persistence:
  enabled: true
  persistentVolumeClaim:
    storageClassName: nfs
    storage: 10Gi
    accessMode: ReadWriteMany
```

Using `ReadWriteMany` enables all replicas to access a shared provider and binary cache, preventing each replica from re-downloading providers and binaries and warming up its own cache, as happens with the default ephemeral volume.

You can also use `ReadWriteOnce` in a multi-replica setup, but it limits all replicas to a single node where the PVC is mounted.

#### Use Its Own PVC

If `persistence.enabled` is `true` and `persistentVolumeClaim.claimName` is empty, a PVC is created with the chart's full name (e.g., `{release-name}-agent-local`). To configure an existing PVC, use the `persistentVolumeClaim.claimName` option.

For more details, see the [Kubernetes storage documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).

## Graceful Termination

The agent's default termination mode is set to `grace-shutdown` via the `SCALR_AGENT_WORKER_ON_STOP_ACTION` configuration.
In this mode, the agent redirects the `SIGTERM` signal immediately to the underlying process (like OpenTofu/Terraform) and waits for it to complete for `SCALR_AGENT_WORKER_GRACE_SHUTDOWN_TIMEOUT` seconds before shutting down.

Learn more about Scalr Agent termination behavior: https://docs.scalr.io/docs/configuration#scalr_agent_worker_on_stop_action

The application layer graceful termination timeout ([SCALR_AGENT_WORKER_GRACE_SHUTDOWN_TIMEOUT](https://docs.scalr.io/docs/configuration#scalr_agent_worker_grace_shutdown_timeout)) is calculated dynamically from Kubernetes' `terminationGracePeriodSeconds` minus 10 seconds to give the agent additional time to terminate and push task results to the Scalr platform. As a result, `terminationGracePeriodSeconds` must be at least 10 seconds.

## Troubleshooting

If you encounter internal system errors or unexpected behavior, please open a Scalr Support request at [Scalr Support Center](https://scalr-labs.atlassian.net/servicedesk/customer/portal/31).

Before doing so, enable debug logs using the `extraEnv.SCALR_AGENT_DEBUG` option. Then collect the debug-level application logs covering the time window when the incident occurred, and attach them to your support ticket.

To archive all logs from the Scalr agent namespace in a single bundle, replace the `ns` variable with the name of your Helm release namespace and run:

```shell
ns="scalr-agent"
mkdir -p logs && for pod in $(kubectl get pods -n $ns -o name); do kubectl logs -n $ns $pod > "logs/${pod##*/}.log"; done && zip -r agent-local-logs.zip logs && rm -rf logs
```

It's best to pull the logs immediately after an incident, since this command will not retrieve logs from restarted or terminated pods.

---

**Homepage:** <https://github.com/Scalr/agent-helm/tree/master/charts/agent-local>

{{ template "chart.maintainersSection" . }}

{{ template "chart.requirementsSection" . }}

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}
